# Challenge 1b: Multi-Collection PDF Analysis

## Overview
Advanced PDF analysis solution that processes multiple document collections and extracts relevant content based on specific personas and use cases.

## Project Structure
```
Challenge_1b/
├── Collection 1/                           # Travel Planning
│   ├── PDFs/                              # South of France travel guides (7 PDFs)
│   ├── challenge1b_input.json             # Persona + Task + Documents
│   └── challenge1b_output.json            # Output generated by your script
│
├── Collection 2/                           # Adobe Acrobat Learning
│   ├── PDFs/                              # Acrobat tutorials
│   ├── challenge1b_input.json
│   └── challenge1b_output.json
│
├── Collection 3/                           # Recipe Collection
│   ├── PDFs/                              # Vegetarian recipes, menus, etc.
│   ├── challenge1b_input.json
│   └── challenge1b_output.json
│
├── challenge1b.py                         # ✅ Final Python script for all collections
├── Dockerfile                             # ✅ Dockerfile for containerized execution
└── README.md                              # ✅ Instructions + format + usage guide

```

## Collections

### Collection 1: Travel Planning
- **Challenge ID**: round_1b_002
- **Persona**: Travel Planner
- **Task**: Plan a 4-day trip for 10 college friends to South of France
- **Documents**: 7 travel guides

### Collection 2: Adobe Acrobat Learning
- **Challenge ID**: round_1b_003
- **Persona**: HR Professional
- **Task**: Create and manage fillable forms for onboarding and compliance
- **Documents**: 15 Acrobat guides

### Collection 3: Recipe Collection
- **Challenge ID**: round_1b_001
- **Persona**: Food Contractor
- **Task**: Prepare vegetarian buffet-style dinner menu for corporate gathering
- **Documents**: 9 cooking guides

## Input/Output Format

### Input JSON Structure
```json
{
  "challenge_info": {
    "challenge_id": "round_1b_XXX",
    "test_case_name": "specific_test_case"
  },
  "documents": [{"filename": "doc.pdf", "title": "Title"}],
  "persona": {"role": "User Persona"},
  "job_to_be_done": {"task": "Use case description"}
}
```

### Output JSON Structure
```json
{
  "metadata": {
    "input_documents": ["list"],
    "persona": "User Persona",
    "job_to_be_done": "Task description"
  },
  "extracted_sections": [
    {
      "document": "source.pdf",
      "section_title": "Title",
      "importance_rank": 1,
      "page_number": 1
    }
  ],
  "subsection_analysis": [
    {
      "document": "source.pdf",
      "refined_text": "Content",
      "page_number": 1
    }
  ]
}
```
---
### ⚙️ How It Works

- The script extracts keywords from the job_to_be_done.
- Each PDF is scanned using PyMuPDF, extracting text block-by-block.
- Each block is scored based on how many keywords it contains.
- Top 5 matching sections are ranked and written to extracted_sections.
- The corresponding text is refined and added to subsection_analysis.

---

### 🐳 Docker Usage
🛠️ Build Docker Image
```
docker build --platform linux/amd64 -t challenge1b .
```

### 📌 Example Run
To run analysis for Collection 1, just run:
```
docker run --rm \
  -v $(pwd)/Challenge_1b/Collection\ 1:/app/input \
  -v $(pwd)/Challenge_1b/Collection\ 1:/app/output \
  --network none \
  challenge1b
```
## Key Features
- Persona-based content analysis
- Importance ranking of extracted sections
- Multi-collection document processing
- Structured JSON output with metadata

---

**Note**: This README provides a brief overview of the Challenge 1b solution structure based on available sample data. 